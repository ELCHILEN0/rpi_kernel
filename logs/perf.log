Test Setup:
a = (60x200); b = (200x40)

// Single Core Execution N=200
(0, 0) -> (40, 60)
(0, 0) -> (40, 60)
(0, 0) -> (40, 60)
(0, 0) -> (40, 60)
done 0
3   [core 0] exiting
usr_count: 135033498, sys_count: 46
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
done 1
4   [core 0] exiting
usr_count: 134944268, sys_count: 52
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
done 3
6   [core 0] exiting
usr_count: 134998302, sys_count: 46
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
done 2
5   [core 0] exiting
usr_count: 135001320, sys_count: 46
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0


// Multi Core Execution...
(0, 0) -> (40, 60)
(0, 0) -> (40, 60)
(0, 0) -> (40, 60)
(0, 0) -> (40, 60)
done 0
6   [core 2] exiting
usr_count: 0, sys_count: 0
usr_count: 57600092, sys_count: 138
usr_count: 569439164, sys_count: 94
usr_count: 0, sys_count: 0
done 1
done 3
done 2
12  [core 0] exiting
usr_count: 135858358, sys_count: 58
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
11  [core 3] exiting
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
usr_count: 0, sys_count: 0
usr_count: 135886128, sys_count: 46
8   [core 1] exiting
usr_count: 0, sys_count: 0
usr_count: 78366992, sys_count: 94
usr_count: 57600152, sys_count: 128
usr_count: 0, sys_count: 0

// Results...
[ // singlecore
135033498,
134944268,
134998302,
135001320,
]

[ // multicore
627039256,
135858358,
135886128,
135967144
]

// Note, HUGE difference in core 0, is it bad metrics, perhaps contention with __spinlock.newlib...
// Also note that all the processes have about LOSS of (~.7%), since all matrices are operating on the same region of memory,
// there is minimal thrashing (except 0?), has it gotten ahead of the first cores then begins to continuously thrash? 